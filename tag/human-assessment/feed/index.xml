<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>human assessment &#8211; Amin Hosseiny Marani, PhD Student</title>
	<atom:link href="http://localhost/amin/tag/human-assessment/feed/" rel="self" type="application/rss+xml" />
	<link></link>
	<description></description>
	<lastBuildDate>Fri, 24 Jun 2022 13:20:46 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0</generator>
	<item>
		<title>More than Good and Bad: Human Assessments of Machine Labeling Quality Have Multiple Dimensions</title>
		<link>/2022/06/23/more-than-good-and-bad-human-assessments-of-machine-labeling-quality-have-multiple-dimensions/</link>
		
		<dc:creator><![CDATA[aminhosseiny]]></dc:creator>
		<pubDate>Thu, 23 Jun 2022 19:32:57 +0000</pubDate>
				<category><![CDATA[Projects]]></category>
		<category><![CDATA[assessment]]></category>
		<category><![CDATA[bias]]></category>
		<category><![CDATA[human assessment]]></category>
		<category><![CDATA[human-centered AI]]></category>
		<category><![CDATA[nlp]]></category>
		<category><![CDATA[topic labeling]]></category>
		<category><![CDATA[topic modeling]]></category>
		<guid isPermaLink="false">/?p=53</guid>

					<description><![CDATA[This project develops a novel measure for human assessments of quality in machine labeling tasks. The paper tests this measure across two studies, one using an unsupervised task (generating labels for topic models) and one using a supervised task (labeling framing in political news coverage). For each label, study participants responded to several items asking [&#8230;]]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image"><img src="/wp-content/uploads/2022/06/objectionable_tech_s2.svg" alt="" class="wp-image-54"/></figure>



<p>This project  develops a novel measure for human assessments of quality in machine labeling tasks. The paper tests this measure across two studies, one using an unsupervised task (generating labels for topic models) and one using a supervised task (labeling framing in political news coverage). For each label, study participants responded to several items asking them to assess each label according to a variety of different criteria.</p>



<p><br>Exploratory factor analysis of these items reveals a two-factor latent structure in participants&#8217; assessments of label quality that is consistent across both studies. Subsequent analysis demonstrates that this multi-item, two-factor measure can reveal nuances that would be missed using either a single-item measure of perceived label quality or established calculable performance metrics. The paper concludes by suggesting future directions for the development of human-centered approaches to evaluating NLP and ML systems more broadly.</p>



<p><em>This paper will be submitted soon&#8230;</em></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>One Rating to Rule Them All? Evidence of Multidimensionality in Human Assessment of Topic Labeling Quality.</title>
		<link>/2022/06/23/one-rating-to-rule-them-all-evidence-of-multidimensionality-in-human-assessment-of-topic-labeling-quality/</link>
		
		<dc:creator><![CDATA[aminhosseiny]]></dc:creator>
		<pubDate>Thu, 23 Jun 2022 17:09:51 +0000</pubDate>
				<category><![CDATA[Projects]]></category>
		<category><![CDATA[assessment]]></category>
		<category><![CDATA[human assessment]]></category>
		<category><![CDATA[human-centered AI]]></category>
		<category><![CDATA[nlp]]></category>
		<category><![CDATA[topic labeling]]></category>
		<category><![CDATA[topic modeling]]></category>
		<guid isPermaLink="false">/?p=26</guid>

					<description><![CDATA[Two general approaches are common for evaluating automatically generated labels in topic modeling: direct human assessment; or performance metrics that can be calculated without, but still correlate with, human assessment. However, both approaches implicitly assume that the quality of a topic label is single-dimensional.In contrast, this project provides evidence that human assessments about the quality [&#8230;]]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-cover is-light" style="min-height:530px"><span aria-hidden="true" class="wp-block-cover__background has-background-dim-0 has-background-dim"></span><img class="wp-block-cover__image-background wp-image-30" alt="" src="/wp-content/uploads/2022/06/suitable_tech_s1-1.svg" data-object-fit="cover"/><div class="wp-block-cover__inner-container"></div></div>



<p>Two general approaches are common for evaluating automatically generated labels in topic modeling: direct human assessment; or performance metrics that can be calculated without, but still correlate with, human assessment. However, both approaches implicitly assume that the quality of a topic label is single-dimensional.<br>In contrast, this project provides evidence that human assessments about the quality of topic labels consist of multiple latent dimensions. This evidence comes from human assessments of four simple labeling techniques.</p>



<p><br>For each label, study participants responded to several items asking them to assess each label according to a variety of different criteria.<br>Exploratory factor analysis shows that these human assessments of labeling quality have a two-factor latent structure. Subsequent analysis demonstrates that this multi-item, two-factor assessment can reveal nuances that would be missed using either a single-item human assessment of perceived label quality or established performance metrics. The paper concludes by suggesting future directions for the development of human-centered approaches to evaluating NLP and ML systems more broadly.</p>



<p><em>Paper has been submitted to CIKM 2022.</em></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Bias as a Distinct Factor in Human Ratings of Machine Labelingï¿¼</title>
		<link>/2022/06/23/bias-as-a-distinct-factor-in-human-ratings-of-machine-labeling/</link>
		
		<dc:creator><![CDATA[aminhosseiny]]></dc:creator>
		<pubDate>Thu, 23 Jun 2022 14:06:16 +0000</pubDate>
				<category><![CDATA[Projects]]></category>
		<category><![CDATA[bias]]></category>
		<category><![CDATA[human assessment]]></category>
		<category><![CDATA[nlp]]></category>
		<category><![CDATA[topic labeling]]></category>
		<category><![CDATA[topic modeling]]></category>
		<guid isPermaLink="false">/?p=18</guid>

					<description><![CDATA[This project argue that human assessments of machine labeling can reveal bias as a distinct measure separate from other perceptions of label quality. Human subjects were asked to assess the quality of automatically generated labels for a trained topic model. Quality assessments were gathered using 15 distinct self-report questions. Exploratory factor analysis identified a distinct [&#8230;]]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" src="/wp-content/uploads/2022/06/image.png" alt="" class="wp-image-20" width="254" height="293" srcset="/wp-content/uploads/2022/06/image.png 468w, /wp-content/uploads/2022/06/image-260x300.png 260w" sizes="(max-width: 254px) 100vw, 254px" /></figure>



<p>This project argue that human assessments of machine labeling can reveal bias as a distinct measure separate from other perceptions of label quality. Human subjects were asked to assess the quality of automatically generated labels for a trained topic model. Quality assessments were gathered using 15 distinct self-report questions. Exploratory factor analysis identified a distinct &#8220;bias&#8221; factor. This point is likely relevant for a wide variety of machine labeling tasks.</p>



<p><a href="https://github.com/aminmarani/aminmarani.github.io/raw/master/Bias_as_a_Distinct_Factor_in_Human_Rating_of_Machine_Learning.pdf" target="_blank" rel="noreferrer noopener">Want to read more?</a> </p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
